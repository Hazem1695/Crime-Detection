{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3670114,"sourceType":"datasetVersion","datasetId":1704076},{"sourceId":4769020,"sourceType":"datasetVersion","datasetId":2760272},{"sourceId":8160313,"sourceType":"datasetVersion","datasetId":4827796},{"sourceId":8161496,"sourceType":"datasetVersion","datasetId":4828682},{"sourceId":8221377,"sourceType":"datasetVersion","datasetId":4873973},{"sourceId":8221879,"sourceType":"datasetVersion","datasetId":4874298},{"sourceId":8330237,"sourceType":"datasetVersion","datasetId":4946359},{"sourceId":8330559,"sourceType":"datasetVersion","datasetId":4946558},{"sourceId":8341375,"sourceType":"datasetVersion","datasetId":4954360},{"sourceId":8341839,"sourceType":"datasetVersion","datasetId":4954680},{"sourceId":8388130,"sourceType":"datasetVersion","datasetId":4989043}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/docs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport keras\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications.densenet import DenseNet121\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom tensorflow_docs.vis import embed\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport imageio\nimport cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Version ","metadata":{}},{"cell_type":"code","source":"import keras\nprint(keras.__version__)\n\nimport tensorflow as tf\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define hyperparameters","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LENGTH = 20\nNUM_FEATURES = 1024\nIMG_SIZE = 128\nEPOCHS = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preparation\n- Reduce the image size to 128x128 instead to spead up\n- Use a pre-trained DenseNet121 for feature extraction\n- directly pad shorter videos to length MAX_SEQ_LENGTH","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\n\nprint(f\"Total videos for training: {len(train_df)}\")\nprint(f\"Total videos for testing: {len(test_df)}\")\n\ncenter_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n\n\ndef crop_center(frame):\n    cropped = center_crop_layer(frame[None, ...])\n    cropped = keras.ops.convert_to_numpy(cropped)\n    cropped = keras.ops.squeeze(cropped)\n    return cropped\n\n\n\n# Following method is modified from this tutorial:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\ndef load_video(path, max_frames=0, offload_to_cpu=False):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = frame[:, :, [2, 1, 0]]\n            frame = crop_center(frame)\n            if offload_to_cpu and keras.backend.backend() == \"torch\":\n                frame = frame.to(\"cpu\")\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    if offload_to_cpu and keras.backend.backend() == \"torch\":\n        return np.array([frame.to(\"cpu\").numpy() for frame in frames])\n    return np.array(frames)\n\n\ndef build_feature_extractor():\n    feature_extractor = DenseNet121(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.densenet.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\n\nfeature_extractor = build_feature_extractor()\n\n\n# Label preprocessing with StringLookup.\nlabel_processor = keras.layers.StringLookup(\n    num_oov_indices=0, vocabulary=np.unique(train_df[\"Label\"]), mask_token=None\n)\nprint(label_processor.get_vocabulary())\n\n\ndef prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"Video_Path\"].values.tolist()\n    labels = df[\"Label\"].values\n    labels = label_processor(labels[..., None]).numpy()\n\n    # `frame_features` are what we will feed to our sequence model.\n    frame_features = np.zeros(\n        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n    )\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n\n        # Pad shorter videos.\n        if len(frames) < MAX_SEQ_LENGTH:\n            diff = MAX_SEQ_LENGTH - len(frames)\n            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n            frames = np.concatenate(frames, padding)\n\n        frames = frames[None, ...]\n\n        # Initialize placeholder to store the features of the current video.\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                if np.mean(batch[j, :]) > 0.0:\n                    temp_frame_features[i, j, :] = feature_extractor.predict(\n                        batch[None, j, :]\n                    )\n\n                else:\n                    temp_frame_features[i, j, :] = 0.0\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n\n    return frame_features, labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show Sample From train_df","metadata":{}},{"cell_type":"code","source":"train_df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Paths","metadata":{}},{"cell_type":"code","source":"# Function to check if paths exist\ndef check_paths(df, column_name):\n    corrupted_paths = []\n    for path in df[column_name]:\n        if not os.path.exists(path):\n            corrupted_paths.append(path)\n    return corrupted_paths\n\n# Check paths in train_df and test_df\ntrain_corrupted = check_paths(train_df, 'Video_Path')\ntest_corrupted = check_paths(test_df, 'Video_Path')\n\n# Print corrupted paths\nprint(\"Corrupted paths in train_df:\")\nfor path in train_corrupted:\n    print(path)\n\nprint(\"\\nCorrupted paths in test_df:\")\nfor path in test_corrupted:\n    print(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display Sample From Dataset","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_sample_data(df, root_dir, num_samples=5):\n    # Randomly select 'num_samples' videos\n    sample_df = df.sample(n=num_samples)\n    video_paths = sample_df['Video_Path'].values\n    labels = sample_df['Label'].values\n    \n    for video_path, label in zip(video_paths, labels):\n        frames = load_video(os.path.join(root_dir, video_path))\n        \n        # Skip videos with no frames\n        if len(frames) == 0:\n            print(f\"Skipping video at path {video_path} because it has no frames.\")\n            continue\n        \n        # Display some information about the video\n        print(f\"Video Path: {video_path}\")\n        print(f\"Label: {label}\")\n        print(f\"Number of Frames: {len(frames)}\")\n        \n        # Display the first few frames from the video\n        fig, axes = plt.subplots(1, min(len(frames), 5), figsize=(15, 3))\n        fig.suptitle(f\"Sample Frames - Label: {label}\")\n        for j, frame in enumerate(frames[:5]):\n            axes[j].imshow(frame)\n            axes[j].axis('off')\n        plt.show()\n\n# Call the function with the training DataFrame and the root directory of your videos\nshow_sample_data(test_df, 'test', num_samples=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract Feature From Train data and Test data\n- After Extract Feature save it .npy file  ","metadata":{}},{"cell_type":"code","source":"train_data, train_labels = prepare_all_videos(train_df, \"name_of_dataset_dirictory\")\nnp.save(\"train_data.npy\", train_data)\nnp.save(\"train_labels.npy\", train_labels)\ntest_data, test_labels = prepare_all_videos(test_df, \"name_of_dataset_dirictory\")\nnp.save(\"test_data.npy\", test_data)\nnp.save(\"test_labels.npy\", test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load .npy files ","metadata":{}},{"cell_type":"code","source":"train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\ntest_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\nprint(f\"Frame features in train set: {train_data.shape}\")\nprint(f\"Frame features in train set: {test_data.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Transformer","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim\n        )\n        self.sequence_length = sequence_length\n        self.output_dim = output_dim\n\n    def build(self, input_shape):\n        self.position_embeddings.build(input_shape)\n\n    def call(self, inputs):\n        # The inputs are of shape: `(batch_size, frames, num_features)`\n        inputs = keras.ops.cast(inputs, self.compute_dtype)\n        length = keras.ops.shape(inputs)[1]\n        positions = keras.ops.arange(start=0, stop=length, step=1)\n        embedded_positions = self.position_embeddings(positions)\n        return inputs + embedded_positions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now, we can create a subclassed layer for the Transformer.","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n        )\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=keras.activations.gelu),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs, mask=None):\n        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions for Training","metadata":{}},{"cell_type":"code","source":"def get_compiled_model(shape):\n    sequence_length = MAX_SEQ_LENGTH\n    embed_dim = NUM_FEATURES\n    dense_dim = 4\n    num_heads = 1\n    classes = len(label_processor.get_vocabulary())\n\n    inputs = keras.Input(shape=shape)\n    x = PositionalEmbedding(\n        sequence_length, embed_dim, name=\"frame_position_embedding\"\n    )(inputs)\n    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n    x = layers.GlobalMaxPooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n\ndef run_experiment():\n    filepath = \"/tmp/video_classifier.weights.h5\"\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        filepath, save_weights_only=True, save_best_only=True, verbose=1\n    )\n\n    model = get_compiled_model(train_data.shape[1:])\n    history = model.fit(\n        train_data,\n        train_labels,\n        validation_split=0.15,\n        epochs=EPOCHS,\n        callbacks=[checkpoint],\n    )\n\n    model.load_weights(filepath)\n    _, accuracy = model.evaluate(test_data, test_labels)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    \n    training_accuracy = history.history['accuracy'][-1]\n    print(f\"Training accuracy: {round(training_accuracy * 100, 2)}%\")\n    \n    \n    # Predict test labels\n    test_pred = model.predict(test_data)\n    test_pred_classes = np.argmax(test_pred, axis=1)\n\n    # Generate confusion matrix\n    cm = confusion_matrix(test_labels, test_pred_classes)\n\n    # Print classification report\n    print(\"Classification Report:\")\n    print(classification_report(test_labels, test_pred_classes, target_names=label_processor.get_vocabulary(), zero_division=1))\n\n    # Calculate accuracy for each class\n    class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n    class_accuracy_dict = {label: acc * 100 for label, acc in zip(label_processor.get_vocabulary(), class_accuracy)}\n    print(\"Accuracy for each class:\")\n    print(class_accuracy_dict)\n\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training and inference","metadata":{}},{"cell_type":"code","source":"trained_model = run_experiment()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Model On Single Video From Test dataset","metadata":{}},{"cell_type":"code","source":"def prepare_single_video(frames):\n    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    # Pad shorter videos.\n    if len(frames) < MAX_SEQ_LENGTH:\n        diff = MAX_SEQ_LENGTH - len(frames)\n        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n        frames = np.concatenate(frames, padding)\n\n    frames = frames[None, ...]\n\n    # Extract features from the frames of the current video.\n    for i, batch in enumerate(frames):\n        video_length = batch.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n        for j in range(length):\n            if np.mean(batch[j, :]) > 0.0:\n                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n            else:\n                frame_features[i, j, :] = 0.0\n\n    return frame_features\n\n\ndef predict_action(path):\n    class_vocab = label_processor.get_vocabulary()\n\n    frames = load_video(os.path.join(\"test\", path), offload_to_cpu=True)\n    frame_features = prepare_single_video(frames)\n    probabilities = trained_model.predict(frame_features)[0]\n\n    plot_x_axis, plot_y_axis = [], []\n\n    for i in np.argsort(probabilities)[::-1]:\n        plot_x_axis.append(class_vocab[i])\n        plot_y_axis.append(probabilities[i])\n        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n        \n    plt.figure(figsize=(18, 6))\n    plt.bar(plot_x_axis, plot_y_axis, label=plot_x_axis)\n    plt.xlabel(\"class_label\")\n    plt.xlabel(\"Probability\")\n    plt.show()\n\n    return frames\n\n\n# This utility is for visualization.\n# Referenced from:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\ndef to_gif(images):\n    converted_images = images.astype(np.uint8)\n    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n    return embed.embed_file(\"animation.gif\")\n\n\ntest_video = np.random.choice(test_df[\"Video_Path\"].values.tolist())\nprint(f\"Test video path: {test_video}\")\ntest_frames = predict_action(test_video)\nto_gif(test_frames[:MAX_SEQ_LENGTH])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}